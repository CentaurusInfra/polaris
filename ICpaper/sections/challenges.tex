%!TEX root = ../main.tex
\section{Research Challenges}
\label{sec:Challenges}

\begin{itemize}
	\item [\textbf{RC-1}] -- 
	Currently most of cloud platforms provide a very rudimentary
	support for specifying QoS and SLOs to end users. The vast majority 
	of models is based on the concepts of resource quotas, such as
	requests/limits in Kubernetes or templates/types in AWS. Unfortunately,
	correctly mapping desired workload's performance model to 
	resource quotas is a very challenging task in practice.
	
	\item [\textbf{RC-2}] -- 
	Vast majority of cloud platforms allow for specifying resource guarantees,
	but very few offer any support for additional elasticity dimensions,
	such as cost or quality. This significantly limits the end user (consumer)
	who might care more to optimize for the costs of executing a specific workload.
	Unfortunately, apart from some niche solutions (e.g., AWS Spot Instances)
	there is very limited support for multi-dimensional elasticity concerns 
	and optimization.
	
	\item [\textbf{RC-3}] -- 
	Latency-sensitive applications greatly benefit from NUMA aware 
	resource allocation. Other types of workloads such as CUP-bound, 
	ML-heavy workloads can benefit from CPU affinity such as GPU pinning.
	While there is usually a general support for node affinity, the 
	end user needs to	deal with too many low level details such
	as writing node selection expressions, which usually relay 
	on cluster specific features, hence making workloads hardly portable.
	Additionally, specifying such QoS requirements is very challenging task
	as it requires deep knowledge about both infrastructure setup specifics
	and cloud platform.
	
	\item [\textbf{RC-4}] -- 
	Observability is foundational element that needs to be considered 
	when building modern cloud-native and microservice applications.
	\todo{Finish}
	
	\item [\textbf{RC-5}] -- 
	%Cloud computing promises easy and quick consumption of infrastructure resources. 
	While there are numerous tools supporting infrastructure 
	provisioning and configuration management, support for run time elastic
	controlling remains fairly limited. It mainly bases on a notion 
	of autoscaling groups/sets of resources where a user specifies their cardinality.
	Most of the solutions only allow for service-level scaling policies 
	as opposed to full-stack scaling strategies. On the one hand, this 
	makes dealing with elasticity concerns cumbersome. On the other hand,
	synchronizing scaling actions across multiple services and 
	defining consistent and congruent scaling strategies becomes 
	very difficult. Finally, optimizing scaling actions 
	for specific elasticity and SLO dimensions, such as cost,	
	has only a rudimentary support.
	%eg AWS AutoScaling (but only a few templates with target scaling strategy...}
	Therefore, creating suitable elastic scaling strategies to date remains
	a challenging task.
	
	%\todo{Finish. Policies must be specified pro service
	%(application deployment segment), synchronizing scaling actions 
	%of multiple services is hard (consistent and congruent scaling strategies), 
	%optimizing for specific SLO such as cost 
	%is little supported eg AWS AutoScaling (but only a few 
	%templates with target scaling strategy...}	
\end{itemize}